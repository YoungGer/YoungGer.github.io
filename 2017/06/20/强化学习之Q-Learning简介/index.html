<!doctype html>
<html class="theme-next ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DQN,Q-table,强化学习," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description" content="强化学习在alphago中大放异彩，本文将简要介绍强化学习的一种q-learning。先从最简单的q-table下手，然后针对state过多的问题引入q-network，最后通过两个例子加深对q-learning的理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习之Q-Learning简介">
<meta property="og:url" content="http://younggy.com/2017/06/20/强化学习之Q-Learning简介/index.html">
<meta property="og:site_name" content="YoungGy">
<meta property="og:description" content="强化学习在alphago中大放异彩，本文将简要介绍强化学习的一种q-learning。先从最简单的q-table下手，然后针对state过多的问题引入q-network，最后通过两个例子加深对q-learning的理解。">
<meta property="og:image" content="http://img.blog.csdn.net/20170620115607588?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170620115607588?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170620120100230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170620120111481?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170620120124841?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170620164751795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2017-11-27T06:30:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习之Q-Learning简介">
<meta name="twitter:description" content="强化学习在alphago中大放异彩，本文将简要介绍强化学习的一种q-learning。先从最简单的q-table下手，然后针对state过多的问题引入q-network，最后通过两个例子加深对q-learning的理解。">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post',
    motion: false
  };
</script>

  <title> 强化学习之Q-Learning简介 | YoungGy </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?your-analytics-id";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">YoungGy</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">心有猛虎 细嗅蔷薇</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tools">
          <a href="/tools" rel="section">
            
              <i class="menu-item-icon fa fa-question-circle fa-fw"></i> <br />
            
            工具
          </a>
        </li>
      

      
      
    </ul>
  

  
    <div class="site-search">
      
  
  <form class="site-search-form">
    <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
  </form>


<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'owv-2W-dWzRWk1KttzWi','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                强化学习之Q-Learning简介
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            发表于
            <time itemprop="dateCreated" datetime="2017-06-20T19:23:40+08:00" content="2017-06-20">
              2017-06-20
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分类于
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/06/20/强化学习之Q-Learning简介/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/06/20/强化学习之Q-Learning简介/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p><img src="http://img.blog.csdn.net/20170620115607588?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="500" alt="图片名称" align="center"></p>
<p>强化学习在alphago中大放异彩，本文将简要介绍强化学习的一种q-learning。先从最简单的q-table下手，然后针对state过多的问题引入q-network，最后通过两个例子加深对q-learning的理解。</p>
<a id="more"></a>
<h1 id="强化学习">强化学习</h1><p>强化学习通常包括两个实体<code>agent</code>和<code>environment</code>。两个实体的交互如下，在<code>environment</code>的<code>state</code> $s_t$下，<code>agent</code>采取<code>action</code> $a_t$进而得到<code>reward</code> $r_{t}$ 并进入<code>state</code> $s_{t+1}$。</p>
<p><img src="http://img.blog.csdn.net/20170620115607588?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>强化学习的问题，通常有如下特点：</p>
<ul>
<li>不同的action产生不同的reward</li>
<li>reward有延迟性</li>
<li>对某个action的reward是基于当前的state的</li>
</ul>
<h1 id="Q-learning">Q-learning</h1><h2 id="Q-Table">Q-Table</h2><p>Q-learning的核心是Q-table。Q-table的行和列分别表示<code>state</code>和<code>action</code>的值，Q-table的值$Q(s,a)$衡量当前state<code>s</code>采取action<code>a</code>到底有多好。</p>
<p><img src="http://img.blog.csdn.net/20170620120100230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="Bellman_Equation">Bellman Equation</h2><p>在训练的过程中，我们使用Bellman Equation去更新Q-table。</p>
<p>$$Q(s,a) = r + γ(max(Q(s’,a’))$$</p>
<p>Bellman Equation解释如下：$Q(s,a)$表示成当前$s$采取$a$后的即时$r$，加上折价$γ$后的最大reward $max(Q(s’,a’)$。</p>
<h2 id="算法">算法</h2><p>根据Bellman Equation，学习的最终目的是得到Q-table，算法如下：</p>
<ol>
<li>外循环模拟次数num_episodes</li>
<li>内循环每次模拟最大步数num_steps</li>
<li>根据当前的state和q-table选择action（可加入随机性）</li>
<li>根据当前的state和action获得下一步的state和reward</li>
<li>更新q-table: Q[s,a] = Q[s,a] + lr<em>(r + y</em>np.max(Q[s1,:]) - Q[s,a])</li>
</ol>
<h2 id="实例">实例</h2><p>以FrozenLake为例，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import lib</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the environment</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement Q-Table learning algorithm</span></span><br><span class="line"><span class="comment">#Initialize table with all zeros</span></span><br><span class="line">Q = np.zeros([env.observation_space.n,env.action_space.n])</span><br><span class="line"><span class="comment"># Set learning parameters</span></span><br><span class="line">lr = <span class="number">.8</span></span><br><span class="line">y = <span class="number">.95</span></span><br><span class="line">num_episodes = <span class="number">2000</span></span><br><span class="line"><span class="comment">#create lists to contain total rewards and steps per episode</span></span><br><span class="line"><span class="comment">#jList = []</span></span><br><span class="line">rList = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment">#Reset environment and get first new observation</span></span><br><span class="line">    s = env.reset()</span><br><span class="line">    rAll = <span class="number">0</span></span><br><span class="line">    d = <span class="keyword">False</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="comment">#The Q-Table learning algorithm</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; <span class="number">99</span>:</span><br><span class="line">        j+=<span class="number">1</span></span><br><span class="line">        <span class="comment">#Choose an action by greedily (with noise) picking from Q table</span></span><br><span class="line">        a = np.argmax(Q[s,:] + np.random.randn(<span class="number">1</span>,env.action_space.n)*(<span class="number">1.</span>/(i+<span class="number">1</span>)))</span><br><span class="line">        <span class="comment">#Get new state and reward from environment</span></span><br><span class="line">        s1,r,d,_ = env.step(a)</span><br><span class="line">        <span class="comment">#Update Q-Table with new knowledge</span></span><br><span class="line">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</span><br><span class="line">        rAll += r</span><br><span class="line">        s = s1</span><br><span class="line">        <span class="keyword">if</span> d == <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment">#jList.append(j)</span></span><br><span class="line">    rList.append(rAll)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Score over time: "</span> +  str(sum(rList)/num_episodes)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Final Q-Table Values"</span></span><br><span class="line"><span class="keyword">print</span> Q</span><br></pre></td></tr></table></figure>
<h1 id="Deep-Q-learning">Deep-Q-learning</h1><p>q-table存在一个问题，真实情况的state可能无穷多，这样q-table就会无限大，解决这个问题的办法是通过神经网络实现q-table。输入state，输出不同action的q-value。</p>
<p><img src="http://img.blog.csdn.net/20170620120111481?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p><img src="http://img.blog.csdn.net/20170620120124841?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="Experience_replay">Experience replay</h2><p>强化学习由于state之间的相关性存在稳定性的问题，解决的办法是在训练的时候存储当前训练的状态到记忆体$M$，更新参数的时候随机从$M$中抽样mini-batch进行更新。</p>
<p>具体地，$M$中存储的数据类型为 $&lt;s, a, r, s’&gt;$，$M$有最大长度的限制，以保证更新采用的数据都是最近的数据。</p>
<h2 id="Exploration_-_Exploitation">Exploration - Exploitation</h2><ul>
<li>Exploration：在刚开始训练的时候，为了能够看到更多可能的情况，需要对action加入一定的随机性。</li>
<li>Exploitation：随着训练的加深，逐渐降低随机性，也就是降低随机action出现的概率。</li>
</ul>
<h2 id="算法-1">算法</h2><p><img src="http://img.blog.csdn.net/20170620164751795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWW91bmdfR3k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="实例-1">实例</h2><h3 id="CartPole">CartPole</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import lib</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the Cart-Pole game environment</span></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Q-network</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, learning_rate=<span class="number">0.01</span>, state_size=<span class="number">4</span>, </span><br><span class="line">                 action_size=<span class="number">2</span>, hidden_size=<span class="number">10</span>, </span><br><span class="line">                 name=<span class="string">'QNetwork'</span>)</span>:</span></span><br><span class="line">        <span class="comment"># state inputs to the Q-network</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">            self.inputs_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, state_size], name=<span class="string">'inputs'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># One hot encode the actions to later choose the Q-value for the action</span></span><br><span class="line">            self.actions_ = tf.placeholder(tf.int32, [<span class="keyword">None</span>], name=<span class="string">'actions'</span>)</span><br><span class="line">            one_hot_actions = tf.one_hot(self.actions_, action_size)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Target Q values for training</span></span><br><span class="line">            self.targetQs_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>], name=<span class="string">'target'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ReLU hidden layers</span></span><br><span class="line">            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)</span><br><span class="line">            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Linear output layer</span></span><br><span class="line">            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, </span><br><span class="line">                                                            activation_fn=<span class="keyword">None</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">### Train with loss (targetQ - Q)^2</span></span><br><span class="line">            <span class="comment"># output has length 2, for two actions. This next line chooses</span></span><br><span class="line">            <span class="comment"># one value from output (per row) according to the one-hot encoded actions.</span></span><br><span class="line">            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))</span><br><span class="line">            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Experience replay</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_size = <span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.buffer = deque(maxlen=max_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, experience)</span>:</span></span><br><span class="line">        self.buffer.append(experience)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        idx = np.random.choice(np.arange(len(self.buffer)), </span><br><span class="line">                               size=batch_size, </span><br><span class="line">                               replace=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> [self.buffer[ii] <span class="keyword">for</span> ii <span class="keyword">in</span> idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters</span></span><br><span class="line">train_episodes = <span class="number">1000</span>          <span class="comment"># max number of episodes to learn from</span></span><br><span class="line">max_steps = <span class="number">200</span>                <span class="comment"># max steps in an episode</span></span><br><span class="line">gamma = <span class="number">0.99</span>                   <span class="comment"># future reward discount</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Exploration parameters</span></span><br><span class="line">explore_start = <span class="number">1.0</span>            <span class="comment"># exploration probability at start</span></span><br><span class="line">explore_stop = <span class="number">0.01</span>            <span class="comment"># minimum exploration probability </span></span><br><span class="line">decay_rate = <span class="number">0.0001</span>            <span class="comment"># exponential decay rate for exploration prob</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network parameters</span></span><br><span class="line">hidden_size = <span class="number">64</span>               <span class="comment"># number of units in each Q-network hidden layer</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>         <span class="comment"># Q-network learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Memory parameters</span></span><br><span class="line">memory_size = <span class="number">10000</span>            <span class="comment"># memory capacity</span></span><br><span class="line">batch_size = <span class="number">20</span>                <span class="comment"># experience mini-batch size</span></span><br><span class="line">pretrain_length = batch_size   <span class="comment"># number experiences to pretrain the memory</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">mainQN = QNetwork(name=<span class="string">'main'</span>, hidden_size=hidden_size, learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Populate the experience memory</span></span><br><span class="line"><span class="comment"># Initialize the simulation</span></span><br><span class="line">env.reset()</span><br><span class="line"><span class="comment"># Take one random step to get the pole and cart moving</span></span><br><span class="line">state, reward, done, _ = env.step(env.action_space.sample())</span><br><span class="line"></span><br><span class="line">memory = Memory(max_size=memory_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a bunch of random actions and store the experiences</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(pretrain_length):</span><br><span class="line">    <span class="comment"># Uncomment the line below to watch the simulation</span></span><br><span class="line">    <span class="comment"># env.render()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a random action</span></span><br><span class="line">    action = env.action_space.sample()</span><br><span class="line">    next_state, reward, done, _ = env.step(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        <span class="comment"># The simulation fails so no next state</span></span><br><span class="line">        next_state = np.zeros(state.shape)</span><br><span class="line">        <span class="comment"># Add experience to memory</span></span><br><span class="line">        memory.add((state, action, reward, next_state))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Start new episode</span></span><br><span class="line">        env.reset()</span><br><span class="line">        <span class="comment"># Take one random step to get the pole and cart moving</span></span><br><span class="line">        state, reward, done, _ = env.step(env.action_space.sample())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Add experience to memory</span></span><br><span class="line">        memory.add((state, action, reward, next_state))</span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="comment"># Now train with experiences</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">rewards_list = []</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    </span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, train_episodes):</span><br><span class="line">        total_reward = <span class="number">0</span></span><br><span class="line">        t = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> t &lt; max_steps:</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Uncomment this next line to watch the training</span></span><br><span class="line">            env.render() </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Explore or Exploit</span></span><br><span class="line">            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) </span><br><span class="line">            <span class="keyword">if</span> explore_p &gt; np.random.rand():</span><br><span class="line">                <span class="comment"># Make a random action</span></span><br><span class="line">                action = env.action_space.sample()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Get action from Q-network</span></span><br><span class="line">                feed = &#123;mainQN.inputs_: state.reshape((<span class="number">1</span>, *state.shape))&#125;</span><br><span class="line">                Qs = sess.run(mainQN.output, feed_dict=feed)</span><br><span class="line">                action = np.argmax(Qs)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Take action, get new state and reward</span></span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">    </span><br><span class="line">            total_reward += reward</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="comment"># the episode ends so no next state</span></span><br><span class="line">                next_state = np.zeros(state.shape)</span><br><span class="line">                t = max_steps</span><br><span class="line">                </span><br><span class="line">                print(<span class="string">'Episode: &#123;&#125;'</span>.format(ep),</span><br><span class="line">                      <span class="string">'Total reward: &#123;&#125;'</span>.format(total_reward),</span><br><span class="line">                      <span class="string">'Training loss: &#123;:.4f&#125;'</span>.format(loss),</span><br><span class="line">                      <span class="string">'Explore P: &#123;:.4f&#125;'</span>.format(explore_p))</span><br><span class="line">                rewards_list.append((ep, total_reward))</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Add experience to memory</span></span><br><span class="line">                memory.add((state, action, reward, next_state))</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Start new episode</span></span><br><span class="line">                env.reset()</span><br><span class="line">                <span class="comment"># Take one random step to get the pole and cart moving</span></span><br><span class="line">                state, reward, done, _ = env.step(env.action_space.sample())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Add experience to memory</span></span><br><span class="line">                memory.add((state, action, reward, next_state))</span><br><span class="line">                state = next_state</span><br><span class="line">                t += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Sample mini-batch from memory</span></span><br><span class="line">            batch = memory.sample(batch_size)</span><br><span class="line">            states = np.array([each[<span class="number">0</span>] <span class="keyword">for</span> each <span class="keyword">in</span> batch])</span><br><span class="line">            actions = np.array([each[<span class="number">1</span>] <span class="keyword">for</span> each <span class="keyword">in</span> batch])</span><br><span class="line">            rewards = np.array([each[<span class="number">2</span>] <span class="keyword">for</span> each <span class="keyword">in</span> batch])</span><br><span class="line">            next_states = np.array([each[<span class="number">3</span>] <span class="keyword">for</span> each <span class="keyword">in</span> batch])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Train network</span></span><br><span class="line">            target_Qs = sess.run(mainQN.output, feed_dict=&#123;mainQN.inputs_: next_states&#125;)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set target_Qs to 0 for states where episode ends</span></span><br><span class="line">            episode_ends = (next_states == np.zeros(states[<span class="number">0</span>].shape)).all(axis=<span class="number">1</span>)</span><br><span class="line">            target_Qs[episode_ends] = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            targets = rewards + gamma * np.max(target_Qs, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            loss, _ = sess.run([mainQN.loss, mainQN.opt],</span><br><span class="line">                                feed_dict=&#123;mainQN.inputs_: states,</span><br><span class="line">                                           mainQN.targetQs_: targets,</span><br><span class="line">                                           mainQN.actions_: actions&#125;)</span><br><span class="line">        </span><br><span class="line">    saver.save(sess, <span class="string">"checkpoints/cartpole.ckpt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">test_episodes = <span class="number">10</span></span><br><span class="line">test_max_steps = <span class="number">400</span></span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, tf.train.latest_checkpoint(<span class="string">'checkpoints'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, test_episodes):</span><br><span class="line">        t = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> t &lt; test_max_steps:</span><br><span class="line">            env.render() </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Get action from Q-network</span></span><br><span class="line">            feed = &#123;mainQN.inputs_: state.reshape((<span class="number">1</span>, *state.shape))&#125;</span><br><span class="line">            Qs = sess.run(mainQN.output, feed_dict=feed)</span><br><span class="line">            action = np.argmax(Qs)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Take action, get new state and reward</span></span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                t = test_max_steps</span><br><span class="line">                env.reset()</span><br><span class="line">                <span class="comment"># Take one random step to get the pole and cart moving</span></span><br><span class="line">                state, reward, done, _ = env.step(env.action_space.sample())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                state = next_state</span><br><span class="line">                t += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<h3 id="FrozenLake">FrozenLake</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import lib</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># laod env</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Q-Network Approach</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment">#These lines establish the feed-forward part of the network used to choose actions</span></span><br><span class="line">inputs1 = tf.placeholder(shape=[<span class="number">1</span>,<span class="number">16</span>],dtype=tf.float32)</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">16</span>,<span class="number">4</span>],<span class="number">0</span>,<span class="number">0.01</span>))</span><br><span class="line">Qout = tf.matmul(inputs1,W)</span><br><span class="line">predict = tf.argmax(Qout,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.</span></span><br><span class="line">nextQ = tf.placeholder(shape=[<span class="number">1</span>,<span class="number">4</span>],dtype=tf.float32)</span><br><span class="line">loss = tf.reduce_sum(tf.square(nextQ - Qout))</span><br><span class="line">trainer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">updateModel = trainer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set learning parameters</span></span><br><span class="line">y = <span class="number">.99</span></span><br><span class="line">e = <span class="number">0.1</span></span><br><span class="line">num_episodes = <span class="number">2000</span></span><br><span class="line"><span class="comment">#create lists to contain total rewards and steps per episode</span></span><br><span class="line">jList = []</span><br><span class="line">rList = []</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        <span class="comment">#Reset environment and get first new observation</span></span><br><span class="line">        s = env.reset()</span><br><span class="line">        rAll = <span class="number">0</span></span><br><span class="line">        d = <span class="keyword">False</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="comment">#The Q-Network</span></span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="number">99</span>:</span><br><span class="line">            j+=<span class="number">1</span></span><br><span class="line">            <span class="comment">#Choose an action by greedily (with e chance of random action) from the Q-network</span></span><br><span class="line">            a,allQ = sess.run([predict,Qout],feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s:s+<span class="number">1</span>]&#125;)</span><br><span class="line">            <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; e:</span><br><span class="line">                a[<span class="number">0</span>] = env.action_space.sample()</span><br><span class="line">            <span class="comment">#Get new state and reward from environment</span></span><br><span class="line">            s1,r,d,_ = env.step(a[<span class="number">0</span>])</span><br><span class="line">            <span class="comment">#Obtain the Q' values by feeding the new state through our network</span></span><br><span class="line">            Q1 = sess.run(Qout,feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s1:s1+<span class="number">1</span>]&#125;)</span><br><span class="line">            <span class="comment">#Obtain maxQ' and set our target value for chosen action.</span></span><br><span class="line">            maxQ1 = np.max(Q1)</span><br><span class="line">            targetQ = allQ</span><br><span class="line">            targetQ[<span class="number">0</span>,a[<span class="number">0</span>]] = r + y*maxQ1</span><br><span class="line">            <span class="comment">#Train our network using target and predicted Q values</span></span><br><span class="line">            _,W1 = sess.run([updateModel,W],feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s:s+<span class="number">1</span>],nextQ:targetQ&#125;)</span><br><span class="line">            rAll += r</span><br><span class="line">            s = s1</span><br><span class="line">            <span class="keyword">if</span> d == <span class="keyword">True</span>:</span><br><span class="line">                <span class="comment">#Reduce chance of random action as we train the model.</span></span><br><span class="line">                e = <span class="number">1.</span>/((i/<span class="number">50</span>) + <span class="number">10</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        jList.append(j)</span><br><span class="line">        rList.append(rAll)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Percent of succesful episodes: "</span> + str(sum(rList)/num_episodes) + <span class="string">"%"</span></span><br></pre></td></tr></table></figure>
<h1 id="参考资料">参考资料</h1><ol>
<li><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" target="_blank" rel="external">Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks</a></li>
<li>Udacity Deep Learning Nano Degree</li>
</ol>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DQN/" rel="tag">#DQN</a>
          
            <a href="/tags/Q-table/" rel="tag">#Q-table</a>
          
            <a href="/tags/强化学习/" rel="tag">#强化学习</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/18/基于RNN的语言模型与机器翻译NMT/" rel="next" title="基于RNN的语言模型与机器翻译NMT">
                <i class="fa fa-chevron-left"></i> 基于RNN的语言模型与机器翻译NMT
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/03/机器翻译之Facebook的CNN与Google的Attention/" rel="prev" title="机器翻译之Facebook的CNN与Google的Attention">
                机器翻译之Facebook的CNN与Google的Attention <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


        </div>

        


        
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/06/20/强化学习之Q-Learning简介/"
           data-title="强化学习之Q-Learning简介" data-url="http://younggy.com/2017/06/20/强化学习之Q-Learning简介/">
      </div>
    
  </div>


      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/uploads/touxiang.jpg" alt="Guangyao Yang" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Guangyao Yang</p>
        </div>
        <p class="site-description motion-element" itemprop="description">Guangyao Yang</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">39</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">84</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://blog.csdn.net/young_gy" target="_blank">
                  
                    <i class="fa fa-globe"></i> CSDN
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/YoungGer" target="_blank">
                  
                    <i class="fa fa-github"></i> GitHub
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-twitter-url" target="_blank">
                  
                    <i class="fa fa-twitter"></i> Twitter
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-weibo-url" target="_blank">
                  
                    <i class="fa fa-weibo"></i> Weibo
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-douban-url" target="_blank">
                  
                    <i class="fa fa-globe"></i> DouBan
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/ger-young" target="_blank">
                  
                    <i class="fa fa-globe"></i> ZhiHu
                  
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator">
            <i class="fa fa-angle-double-up"></i>
          </div>
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#强化学习"><span class="nav-number">1.</span> <span class="nav-text">强化学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Q-learning"><span class="nav-number">2.</span> <span class="nav-text">Q-learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Q-Table"><span class="nav-number">2.1.</span> <span class="nav-text">Q-Table</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bellman_Equation"><span class="nav-number">2.2.</span> <span class="nav-text">Bellman Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法"><span class="nav-number">2.3.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实例"><span class="nav-number">2.4.</span> <span class="nav-text">实例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Q-learning"><span class="nav-number">3.</span> <span class="nav-text">Deep-Q-learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Experience_replay"><span class="nav-number">3.1.</span> <span class="nav-text">Experience replay</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploration_-_Exploitation"><span class="nav-number">3.2.</span> <span class="nav-text">Exploration - Exploitation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法-1"><span class="nav-number">3.3.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实例-1"><span class="nav-number">3.4.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CartPole"><span class="nav-number">3.4.1.</span> <span class="nav-text">CartPole</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FrozenLake"><span class="nav-number">3.4.2.</span> <span class="nav-text">FrozenLake</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">4.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator">
            <i class="fa fa-angle-double-down"></i>
          </div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guangyao Yang</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"younggy"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     


    
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    motionMiddleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');
      if (CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    };
  });
</script>



  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  
    <script type="text/javascript" src="http://cdn.staticfile.org/mathjax/2.4.0/MathJax.js"></script>
    <script type="text/javascript" src="http://cdn.staticfile.org/mathjax/2.4.0/config/TeX-AMS-MML_HTMLorMML.js"></script>
  


  
  

</body>
</html>
